{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1:\n",
    "\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format\n",
    "Web Scraping has multiple applications across various industries.\n",
    "1. Price Monitoring\n",
    "2. Market Research\n",
    "3. News Monitoring\n",
    "4. Sentiment Analysis\n",
    "5. Email Marketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2:\n",
    "\n",
    "There are different methods used in web scrapping are:\n",
    "    \n",
    "1.Human copy-and-paste\n",
    "2.Text pattern matching\n",
    "3.HTTP programming\n",
    "4.HTML parsing\n",
    "5.DOM parsing\n",
    "6.Semantic annotation recognizing\n",
    "7.Computer vision web-page analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5684d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3:\n",
    "\n",
    "Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from HTML tags, and alter the HTML in the document with which we’re working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1487075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4:\n",
    "\n",
    "Flask is a lightweight framework to build websites. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "\n",
    "The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5:\n",
    "\n",
    "In the web scrapping project we are generally use two service of the aws.\n",
    "1.codepipeline\n",
    "2.Elastic Beanstack\n",
    "\n",
    "CodePipeline is basically used for fetching the data from github repository. it is used to connect the github to the beanstack.\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload our code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
